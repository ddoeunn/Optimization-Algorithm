{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "256px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "1. optimization of multivariate function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khKKKTVIZ8hm",
        "colab_type": "text"
      },
      "source": [
        "# ***OPTIMIZATION OF MULTIVARIATE FUNCTIONS*** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOw7xZ5uZ8ho",
        "colab_type": "text"
      },
      "source": [
        ">* Assume that $f$ is a multivariate real function $f: \\mathbb{R}^p \\rightarrow \\mathbb{R}$  \n",
        "that has only one true or exact minimizer $\\mathbf{x}^* = argmin_{\\mathbf{x} \\in \\mathit{D}}f(\\mathbf{x})$  \n",
        "* Several algorithms for obtaining a sequence $\\{\\mathbf{x}_n\\}_{n \\ge 1}$  \n",
        "that satisfies $\\exists N \\in \\mathbb{N}$ s.t. $\\rVert\\mathbf{x}_n-\\mathbf{x}^*\\rVert < \\epsilon, \\forall n \\ge N$ for a given error bound $\\epsilon > 0$  \n",
        "* For the sequence above, put $\\hat{\\mathbf{x}} = \\mathbf{x}_N$ as an approximated minimizer of $\\mathbf{x}^*$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BObdlot8Z8hp",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **1. Coordinate Descent Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOA-HjCoZ8hq",
        "colab_type": "text"
      },
      "source": [
        "## **Algorithm**  \n",
        "* Iteratively solves univariate minimization problems for minimizing a function  \n",
        "* Given an initial value $\\mathbf{x}_1 = (x_{11}, \\cdots, x_{1p})^{T}$  \n",
        "$f(\\mathbf{x_1}) \\ge f(x_{21}, x_{12}, x_{13}, \\cdots, x_{1p}) \\ge f(x_{21}, x_{22}, x_{13}, \\cdots, x_{1p}) \\ge \\cdots \\ge f(x_{21}, x_{22}, x_{23}, \\cdots, x_{2p})$  \n",
        ",where $x_{21} = argmin_{t \\in \\mathbb{R}}f(t, x_{12}, x_{13}, \\cdots, x_{1p}), \\cdots, \n",
        "x_{2p}= argmin_{t \\in \\mathbb{R}}f(x_{21}, x_{22}, x_{23}, \\cdots, t)$   \n",
        "so that we eventually have $f(\\mathbf{x}_1) \\ge f(\\mathbf{x}_2)$ ,where $\\mathbf{x}_2 = (x_{21}, x_{22}, \\cdots, x_{2p} )^{T}$  \n",
        "\n",
        ">1. Take an initial $\\mathbf{x}_1 = (x_{11}, \\cdots, x_{1p})^{T}$  \n",
        "2. For $n = 1, 2, \\cdots, $  \n",
        "    (1) For $j=1, 2, \\cdots, p$  \n",
        "    $x_{(n+1)j} = argmin_{t \\in \\mathbb{R}}f(x_{(n+1)1}, \\cdots, x_{(n+1)(j-1)}, t, x_{n(j+1)}, \\cdots, x_{np})$  \n",
        "3. If $\\rVert\\mathbf{x}_{n+1}-\\mathbf{x}_n\\rVert < \\epsilon$ then stop, where $\\mathbf{x}_{n+1} = (x_{(n+1)1}, x_{(n+1)2}, \\cdots, x_{(n+1)p})^{T}$  \n",
        "4. Set $\\hat{\\mathbf{x}} = \\mathbf{x}_n$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlMvcYwSZ8hr",
        "colab_type": "text"
      },
      "source": [
        "* **the key step is the minimization in (1)**  \n",
        "    + If it has closed form we are lucky.  \n",
        "    + If not, we need to solve univariate minimization problem.  \n",
        "    + If $f$ is differentiable then (1) can be done by solving $\\frac{df(x_{(n+1)1}, \\cdots, x_{(n+1)(j-1)}, t, x_{n(j+1)}, \\cdots, x_{np})}{dt} = 0$  \n",
        "    \n",
        "* **If $f$ is convex but not differentiable then the algorithm may stuck in a point that is not a local minimizer**  \n",
        ": $f(x, y) = (x-y)^2 +(x+y)^2 + |x-y| + 10|x+y|$  \n",
        "\n",
        "* **Tseng(2001) proved that the CD algorithm always converges to a local minimizer**  \n",
        "**if $f(\\mathbf{x}) = g(\\mathbf{x}) + \\sum^{p}_{j=1}h_j(x_j)$ where $g$ is convex differenctiable and $h_j$ is convex but may not differentiable for all $j \\le p$**  \n",
        ": $f(x, y) = x^2 + y^2 + |x| + 10|y|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbb5XU7bZ8hr",
        "colab_type": "text"
      },
      "source": [
        "## **[Example - LSE]**  \n",
        "* Consider Linear regression model, $\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}$  \n",
        ",where $\\mathbf{y} = (y_1, \\cdots, y_n)^{T}$ is a vector of response, $\\mathbf{X} = (\\mathbf{X}_1, \\cdots, \\mathbf{X}_p)$ is a design matrix, $\\mathbf{\\beta} = (\\beta_1, \\cdots, \\beta_p)^{T}$ is a parameter vector and $\\mathbf{\\epsilon} = (\\epsilon_1, \\cdots, \\epsilon_n)^{T}$ is a random error vector.  \n",
        "* **Least Square Estimator, LSE**  \n",
        ">+ $\\hat{\\mathbf{\\beta}}^{LSE} = argmin_{\\mathbf{\\beta}}L(\\mathbf{\\beta}) = argmin_{\\mathbf{\\beta}}\\rVert\\mathbf{y} - \\mathbf{X\\beta}\\rVert^2 / 2$  \n",
        "> + For each $j \\le p$,  \n",
        "$L(\\mathbf{\\beta}) = (\\mathbf{X}^{T}_j\\mathbf{X}_j)\\beta^2_j/2 - \\mathbf{X}^{T}_j(\\mathbf{y}-\\mathbf{Z}_i\\mathbf{\\alpha}_j)\\beta_j + (\\mathbf{y}-\\mathbf{Z}_j\\mathbf{\\alpha}_j)^{T}(\\mathbf{y}-\\mathbf{Z}_j\\mathbf{\\alpha}_j)/2$  \n",
        "where $\\mathbf{Z}_j = (\\mathbf{X}_j, \\cdots, \\mathbf{X}_{j-1}, \\mathbf{X}_{j+1}, \\cdots, \\mathbf{X}_p)$ and $\\mathbf{\\alpha}_j = (\\beta_1, \\cdots, \\beta_{j-1}, \\beta_{j+1}, \\cdots, \\beta_p)^{T}$  \n",
        "> + Hence, $\\hat{\\beta}^{LSE}_j = \\mathbf{X}^{T}_j(\\mathbf{y}-\\mathbf{Z}_j\\mathbf{\\alpha}_j)/\\mathbf{X}^{T}_j\\mathbf{X}_j$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VvS-_a2Z8hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq0ETSxtZ8hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lseCD(y, X, eps=1e-7, iter_max=1000):\n",
        "    loss = []\n",
        "    p = X.shape[1]\n",
        "    bvec = np.zeros(p)\n",
        "    idx = np.arange(0, p)\n",
        "    for i in range(iter_max):\n",
        "        cur_bvec = bvec.copy()\n",
        "        for j in range(p):\n",
        "            Z = X[:, idx != j]\n",
        "            avec = bvec[idx != j]\n",
        "            dn = np.sum(X[:, j]**2)\n",
        "            up = np.sum(X[:, j] * (y - np.matmul(Z, avec)))\n",
        "            bvec[j] = up / dn\n",
        "        loss.append(np.sum((y - np.matmul(X, bvec))**2))\n",
        "        if np.sum(np.abs(cur_bvec - bvec)) < eps: break\n",
        "    return bvec, loss, i"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOnh7qj7Z8h0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "9e5ba420-632f-4545-a1a1-59922e83cc61"
      },
      "source": [
        "n = 10\n",
        "p = 5\n",
        "bvec = np.ones(p)\n",
        "X = np.random.normal(0, 1, [n, p])\n",
        "y = np.matmul(X, bvec) + np.random.normal(0, 1, n)\n",
        "beta, loss, iteration = lseCD(y, X)\n",
        "beta"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.78676532, 1.29443776, 0.72858145, 1.64515219, 1.32646209])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS30D5puZ8h3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "1e62020c-a5a3-4072-ecd8-3db8459ae067"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph')\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzbI2f6AZ8h7",
        "colab_type": "text"
      },
      "source": [
        "## **[Example - LASSO]**  \n",
        "* **Least Absolute Selection and Shrinkage Opterator, LASSO**  \n",
        "> + $\\hat{\\mathbf{\\beta}}^{LASSO} = argmin_{\\mathbf{\\beta}}L_{\\lambda}(\\mathbf{\\beta}) = argmin_{\\mathbf{\\beta}}\\{\\rVert\\mathbf{y} - \\mathbf{X\\beta}\\rVert^2/2 + \\lambda \\rVert\\mathbf{\\beta}\\rVert_1\\}$  \n",
        ",for some constant $\\lambda > 0$, where $\\lambda$ is often called the tuning parameter.  \n",
        "> + For each $j \\le p$,  \n",
        "$L_{\\lambda}(\\mathbf{\\beta}) = (\\mathbf{X}^{T}_j\\mathbf{X}_j)\\beta^2_j/2 - \\mathbf{X}^T(\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j)\\beta_j + \\lambda|\\beta_j| + (\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j)^T(\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j)/2 + \\lambda\\rVert\\mathbf{\\alpha}_j\\rVert_1$  \n",
        "> + Hence, $\\hat{\\beta}^{LASSO}_j = S_{\\lambda/\\mathbf{X}^T_j\\mathbf{X}_j}(\\mathbf{X}^T_j(\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j) / \\mathbf{X}^T_j\\mathbf{X}_j) = S_{\\lambda/\\mathbf{X}^T_j\\mathbf{X}_j}(\\hat{\\beta}^{LSE}_j)$  \n",
        "where $S_{\\lambda} = sign(x)(|x| - \\lambda)_{+}$ and $c_{+} = cI(c>0)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hmq9DUHZ8h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softThresh(x, lam):\n",
        "    ret = np.sign(x) * (np.abs(x) - lam) * (abs(x) > lam)\n",
        "    return ret"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IISw-_dZ8h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lassoCD(y, X, lam, eps=1e-7, iter_max=1000):\n",
        "    loss = []\n",
        "    p = X.shape[1]\n",
        "    bvec = np.zeros(p)\n",
        "    idx = np.arange(0, p)\n",
        "    for i in range(iter_max):\n",
        "        cur_bvec = bvec.copy()\n",
        "        for j in range(p):\n",
        "            Z = X[:, idx != j]\n",
        "            avec = bvec[idx != j]\n",
        "            dn = np.sum(X[:, j]**2)\n",
        "            up = np.sum(X[:, j] * (y - np.matmul(Z, avec)))\n",
        "            bvec[j] = softThresh(up/dn, lam/dn)\n",
        "        loss.append(np.sum((y - np.matmul(X, bvec))**2))\n",
        "    return bvec, loss, i"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T090bgt2Z8iC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "071fb5f4-3c59-43dc-d0a7-e2a7a2e3cb53"
      },
      "source": [
        "n = 10\n",
        "p = 11\n",
        "bvec = np.ones(p)\n",
        "X = np.random.normal(0, 1, [n, p])\n",
        "y = np.matmul(X, bvec) + np.random.normal(0, 1, n)\n",
        "beta, loss, iteration = lassoCD(y, X, 1) \n",
        "beta"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00000000e+00,  1.52933343e+00,  6.27267306e-04,  4.45234149e-01,\n",
              "        1.10666632e+00,  1.16782619e+00,  1.48874306e+00,  7.78602899e-01,\n",
              "       -0.00000000e+00,  0.00000000e+00,  9.77917939e-01])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW3BGahoZ8iF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "6ac46679-d8cf-4713-873d-e8b64e83a3a1"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W64qoy_sZ8iI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **2. Gradient Descent Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6-54uuKZ8iI",
        "colab_type": "text"
      },
      "source": [
        "***Moving a point for a direction with a stepsize***  \n",
        "* **Univariate function**  \n",
        ">+ The first key idea is to understand \"moving a point $x_n$ to $x_{n+1}$\" as \"choosing an equation\" between (1) and (2)  \n",
        "(1) : $x_{n+1} = x_n + \\alpha \\Rightarrow$ move to the right  \n",
        "(2) : $x_{n+1} = x_n - \\alpha \\Rightarrow$ move to the left  \n",
        "where $\\alpha >0$ is very small positive constant  \n",
        ">+ Hence, when $f$ is differentiable, $x_{n+1} = x_n - \\alpha \\nabla f(x_n)$  \n",
        ">+ Move $x_n$ for a direction $-\\nabla f(x_n)$ with a stepsize $\\alpha > 0$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ_yjRqAZ8iJ",
        "colab_type": "text"
      },
      "source": [
        "* **Multivariate function**  \n",
        ">+ when the function $f$ is multivariate, there are infinitely many directions to move.  \n",
        ">+ **(Direction opposite to current gradient)**  \n",
        ": Assume that $\\nabla f(\\mathbf{x}) \\ne \\mathbf{0}$ for some fixed point $\\mathbf{x}$  \n",
        ": Let $\\mathbf{x}_{\\alpha} = \\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})$ then by the first order Taylor's expansion around $\\mathbf{x}$, we have  \n",
        "$f(\\mathbf{x}_{\\alpha}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T(\\mathbf{x}_{\\alpha}-\\mathbf{x}) + o(\\rVert \\mathbf{x}_{\\alpha}-\\mathbf{x} \\rVert)$  \n",
        "$= f(\\mathbf{x}) - \\alpha \\rVert \\nabla f(\\mathbf{x}) \\rVert^2 + o(\\alpha \\rVert \\nabla f(\\mathbf{x}) \\rVert^2)$  \n",
        "$= f(\\mathbf{x}) - \\alpha \\rVert \\nabla f(\\mathbf{x}) \\rVert^2 + o(\\alpha)$  \n",
        "$\\Rightarrow f(\\mathbf{x}_{\\alpha}) - f(\\mathbf{x}) < -\\alpha \\rVert\\nabla f(\\mathbf{x}) \\rVert^2 < 0$  \n",
        ">+ **(General descent condition)**  \n",
        ": Consider $\\mathbf{x}_{\\alpha}  = \\mathbf{x} + \\alpha \\mathbf{d}$ ,$\\forall \\alpha >0$ where the direction $\\mathbf{d}$ satisfies the descent condition where it makes an angle with $\\nabla f(\\mathbf{x})$ that is greater than 90 degrees,  \n",
        "that is, $\\nabla f(\\mathbf{x})^T\\mathbf{d} < 0$  \n",
        ": By the Taylor's expansion, $f(\\mathbf{x}_{\\alpha}) - f(\\mathbf{x}) < \\alpha \\nabla f(\\mathbf{x})^T\\mathbf{d} < 0$  \n",
        "for all sufficiently small $\\alpha > 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxE1XJSZ8iK",
        "colab_type": "text"
      },
      "source": [
        "## **Algorithm**  \n",
        ">1. Take an initial $\\mathbf{x}_1$  \n",
        ">2. For $n = 1, 2, \\cdots, $  \n",
        "    (1) Choose a direction $\\mathbf{d}_n$ and a stepsize $\\alpha_n$ and set $\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\alpha_n\\mathbf{d}_n$  \n",
        ">3. If $\\rVert \\mathbf{x}_{n+1} - \\mathbf{x}_n \\rVert < \\epsilon$ then stop.  \n",
        ">4. Set $\\hat{\\mathbf{x}} = \\mathbf{x}_n$   \n",
        "\n",
        "* **the key step is (1)**  \n",
        ": If $\\nabla f(\\mathbf{x}_n)^T\\mathbf{d}_n > 0$ then cannot guarantee the descent property.  \n",
        ": If $\\alpha_n$ is too small the convergence speed is too late.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjBbn0vLZ8iK",
        "colab_type": "text"
      },
      "source": [
        "## **Selecting the descent direction**  \n",
        ">* **General form of GD algorithm**  \n",
        ": The most popular form of the GD algorithm is specified by $\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\alpha_n\\mathbf{D}_n\\nabla f(\\mathbf{x}_n)$  \n",
        ",where $\\mathbf{D}_n$ is a strictly positive definite symmetric matrix and $\\alpha_n$ is a stepsize.  \n",
        ": $\\mathbf{d}_n = -\\mathbf{D}_n \\nabla f(\\mathbf{x})$  \n",
        "$~\\nabla f(\\mathbf{x}_n)^T\\mathbf{D}_n\\nabla f(\\mathbf{x}_n) > 0$ because of the strictly positive definiteness of $\\mathbf{D}_n$  \n",
        "\n",
        ">* **Steepest descent method**  \n",
        ": $\\mathbf{D}_n = \\mathbf{I}_p \\Rightarrow \\mathbf{x}_{n+1} = \\mathbf{x}_n - \\alpha \\nabla f(\\mathbf{x}_n)$  \n",
        "$~\\nabla f(\\mathbf{x}_n)^T\\mathbf{d}_n \\ge - \\rVert \\nabla f(\\mathbf{x}_n) \\rVert \\rVert \\mathbf{d}_n \\rVert = -\\rVert \\nabla f(\\mathbf{x}) \\rVert < 0$ ,$\\forall \\rVert\\mathbf{d}_n \\rVert = 1$ by the Schwartz inequality  \n",
        ": Hence, $\\mathbf{d}_n = -\\nabla f(\\mathbf{x}_n) / \\rVert\\nabla f(\\mathbf{x}_n) \\rVert$ where the equality holds for the Schwartz inequality  \n",
        ": the gradient direction is almost orthogonal to the direction for the minimum. Hence, the GD algorithm shows 'zig-zagging' behavior without making fast progress.  \n",
        "\n",
        ">* **Newton's method**  \n",
        ": Assume that $f$ is convex and twice differentiable  \n",
        ": $\\mathbf{D}_n = \\nabla^2f(\\mathbf{x}_n)^{-1} \\Rightarrow \\mathbf{x}_{n+1} = \\mathbf{x}_n -\\alpha_n \\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x})$ ,where $\\nabla^2f(\\mathbf{x}_n)$ is strictly positive definite  \n",
        ": typically converges vary fast and does not exhibit 'zig-zagging' behavior  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLvNwaLpZ8iL",
        "colab_type": "text"
      },
      "source": [
        "## **Newton-Raphson**  \n",
        "* Newton-Raphson algorithm is a gradient descent algorithm when $\\alpha_n = 1$ ,$\\mathbf{D}_n = \\nabla^2f(\\mathbf{x}_n)^{-1} \\Rightarrow \\mathbf{x}_{n+1} = \\mathbf{x}_n - \\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x})$  \n",
        "* Suppose $f : \\mathit{D} \\rightarrow \\mathbb{R}^p$ is convex and twice differentiable, has second derivaton on $\\mathit{D}$  \n",
        "    + The second order approximation of $f$ at $\\mathbf{x}_0$ becomes  \n",
        "    $f(\\mathbf{x}) \\approx q(\\mathbf{x}) = f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^T(\\mathbf{x} - \\mathbf{x}_0) +(\\mathbf{x} - \\mathbf{x}_0)^T\\nabla^2f(\\mathbf{x}_0)(\\mathbf{x} - \\mathbf{x}_0)/2$ around $\\mathbf{x}_n$  \n",
        "    ,where $\\nabla f(\\mathbf{x}_0)$ is the gradient vector of $f$ at $\\mathbf{x}_0$ and  $\\nabla^2f(\\mathbf{x}_0)$ is the Hessian matrix of $f$ at $\\mathbf{x}_0$  \n",
        "    + Minimizing the quadratic approximation $q$ around $\\mathbf{x}_n$  \n",
        "    + By taking the derivative to zero,  \n",
        "    $\\nabla f(\\mathbf{x}_n) = \\nabla^2f(\\mathbf{x}_n)(\\mathbf{x} - \\mathbf{x}_n) = 0  \\Leftrightarrow \\mathbf{x} = \\mathbf{x}_n - \\nabla f(\\mathbf{x}_n)^{-1} \\nabla f(\\mathbf{x}_n)$  \n",
        "* Hence, the Newton-Raphson Algorithm can be thought of a successive minimization of quadratic approximations \n",
        "$\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\alpha \\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPC_UG1XZ8iL",
        "colab_type": "text"
      },
      "source": [
        "## **[Example - LogisticRegression]**  \n",
        "* The estimator(mle) is the minimizer of the logistic loss:  \n",
        "$\\hat{\\mathbf{\\beta}} = argmin_{\\mathbf{\\beta}}L(\\mathbf{\\beta})= argmin_{\\mathbf{\\beta}}\\sum^{n}_{i=1}\\{-y_i \\mathbf{x}^T_i \\mathbf{\\beta} +log(1+exp(\\mathbf{x}^T_i\\mathbf{\\beta}))\\}$  \n",
        "which can be obtained by modeling $f(\\mathbf{x}^T_i\\mathbf{\\beta}) = exp(\\mathbf{x}^T_i\\mathbf{\\beta}) / \\{ 1+exp(\\mathbf{x}^T_i\\mathbf{\\beta})\\}$  \n",
        "* $\\nabla L(\\mathbf{\\beta}) = \\sum^{n}_{i=1}[-y_i + exp(\\mathbf{x}^T_i\\mathbf{\\beta}) / \\{1+exp(\\mathbf{x}^T_i\\mathbf{\\beta})\\}]\\mathbf{x}_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjxgysxuZ8iM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getLoss(y, X, bvec):\n",
        "    xbvec = np.matmul(X, bvec)\n",
        "    loss = -np.sum(y * xbvec) + np.sum(np.log(1 + np.exp(xbvec)))\n",
        "    return loss\n",
        "\n",
        "def getGrad(y, X, bvec):\n",
        "    xbvec = np.minimum(np.matmul(X, bvec), 700)\n",
        "    exbvec = np.exp(xbvec)\n",
        "    pvec = exbvec / (1 + exbvec)\n",
        "    grad = np.matmul(np.transpose(X), (pvec - y))\n",
        "    return grad\n",
        "\n",
        "def logisticGD(y, X, bvec, eps=1e-7, iter_max=1000):\n",
        "    losses = []\n",
        "    for i in range(iter_max):\n",
        "        alpha = .5\n",
        "        gvec = getGrad(y, X, bvec)\n",
        "        loss = getLoss(y, X, bvec)\n",
        "        losses.append(loss)\n",
        "        # stepsize from the backtracking rule\n",
        "        for j in range(iter_max):\n",
        "            nbvec = bvec - alpha * gvec # new bvec\n",
        "            if getLoss(y, X, nbvec) < loss: break\n",
        "            alpha = alpha / 2\n",
        "        if np.sum(np.abs(nbvec - bvec)) < eps: break\n",
        "        bvec = nbvec.copy()\n",
        "    return bvec, losses, i"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLcd0AFMZ8iP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "84ffea8c-dba0-4975-9628-92c2e7d06327"
      },
      "source": [
        "n = 100\n",
        "p = 20\n",
        "X = np.random.normal(0, 1, [n, p])\n",
        "y = np.random.choice([0, 1], n, replace=True)\n",
        "bvec = np.zeros(p)\n",
        "betaGD, loss, iteration = logisticGD(y, X, bvec)\n",
        "betaGD"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.16383516, -0.30204379, -0.06389954,  0.22378696,  0.3484279 ,\n",
              "       -0.07812375, -0.29273194,  0.00678847,  0.0892753 ,  0.07124329,\n",
              "       -0.14949607,  0.13897351,  0.21518069, -0.05456187,  0.03920021,\n",
              "        0.20808057, -0.37380438, -0.00271156,  0.13279272,  0.35532664])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xHzPnZ5Z8iS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "7f5e153d-b6a3-403a-9e06-cb8b3651eb57"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph GD')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxH579NRZ8iU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getHess(y, X, bvec):\n",
        "    xbvec = np.minimum(np.matmul(X, bvec), 700)\n",
        "    exbvec = np.exp(xbvec)\n",
        "    pvec = exbvec / (1 + exbvec)\n",
        "    wvec = pvec * (1 - pvec)\n",
        "    hess = np.matmul(np.matmul(np.transpose(X), np.diag(wvec)), X)\n",
        "    return hess"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2McCgbZkZ8iX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logisticNR(y, X, bvec, eps=1e-7, iter_max=1000):\n",
        "    loss = []\n",
        "    for i in range(iter_max):\n",
        "        loss.append(getLoss(y, X, bvec))\n",
        "        hess = getHess(y, X, bvec)\n",
        "        grad = getGrad(y, X, bvec)\n",
        "        nbvec = bvec - np.matmul(np.linalg.inv(hess), grad) # new bvec\n",
        "        if np.sum(np.abs(nbvec - bvec)) < eps: break\n",
        "        bvec = nbvec\n",
        "    return bvec, loss, i"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu7hHAbbZ8ia",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "1f4eafaa-cea3-491f-eb07-bfb9c36b1b75"
      },
      "source": [
        "betaNR, loss, iteration = logisticNR(y, X, bvec)\n",
        "betaNR"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.16383516, -0.30204379, -0.06389955,  0.22378696,  0.34842791,\n",
              "       -0.07812373, -0.29273194,  0.00678845,  0.08927529,  0.07124329,\n",
              "       -0.14949607,  0.13897351,  0.21518071, -0.05456188,  0.03920022,\n",
              "        0.20808058, -0.37380438, -0.00271156,  0.13279271,  0.35532664])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7oKYvbeZ8id",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "0d39435c-6e88-4d56-ff1b-f533605a6f65"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph NR')\n",
        "plt.show()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbSq2NCWZ8ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UmdlU62Z8ik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "d4760334-70fd-47fd-e11a-c520a120997a"
      },
      "source": [
        "clf = LogisticRegression(penalty='none', solver='newton-cg')\n",
        "clf.fit(X, y)\n",
        "betaLR = clf.coef_.squeeze()\n",
        "betaLR"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.13868964, -0.33507639, -0.00840247,  0.21195501,  0.31014118,\n",
              "       -0.11585419, -0.29417281,  0.00961135,  0.08300741,  0.10201981,\n",
              "       -0.12581409,  0.14605101,  0.19125698, -0.08368352,  0.02633424,\n",
              "        0.22266388, -0.3822354 , -0.05582314,  0.17165245,  0.34971485])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SteSrLXEZ8im",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "outputId": "74a9c616-7111-41a3-87e8-130ce0b77a16"
      },
      "source": [
        "pd.DataFrame([betaGD, betaNR, betaLR], index=['GD', 'NR', 'newton-cg']).transpose()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GD</th>\n",
              "      <th>NR</th>\n",
              "      <th>newton-cg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.163835</td>\n",
              "      <td>-0.163835</td>\n",
              "      <td>-0.138690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.302044</td>\n",
              "      <td>-0.302044</td>\n",
              "      <td>-0.335076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.063900</td>\n",
              "      <td>-0.063900</td>\n",
              "      <td>-0.008402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.223787</td>\n",
              "      <td>0.223787</td>\n",
              "      <td>0.211955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.348428</td>\n",
              "      <td>0.348428</td>\n",
              "      <td>0.310141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.078124</td>\n",
              "      <td>-0.078124</td>\n",
              "      <td>-0.115854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.292732</td>\n",
              "      <td>-0.292732</td>\n",
              "      <td>-0.294173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.006788</td>\n",
              "      <td>0.006788</td>\n",
              "      <td>0.009611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.089275</td>\n",
              "      <td>0.089275</td>\n",
              "      <td>0.083007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.071243</td>\n",
              "      <td>0.071243</td>\n",
              "      <td>0.102020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.149496</td>\n",
              "      <td>-0.149496</td>\n",
              "      <td>-0.125814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.138974</td>\n",
              "      <td>0.138974</td>\n",
              "      <td>0.146051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.215181</td>\n",
              "      <td>0.215181</td>\n",
              "      <td>0.191257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>-0.054562</td>\n",
              "      <td>-0.054562</td>\n",
              "      <td>-0.083684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.039200</td>\n",
              "      <td>0.039200</td>\n",
              "      <td>0.026334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.208081</td>\n",
              "      <td>0.208081</td>\n",
              "      <td>0.222664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-0.373804</td>\n",
              "      <td>-0.373804</td>\n",
              "      <td>-0.382235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>-0.002712</td>\n",
              "      <td>-0.002712</td>\n",
              "      <td>-0.055823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.132793</td>\n",
              "      <td>0.132793</td>\n",
              "      <td>0.171652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.355327</td>\n",
              "      <td>0.355327</td>\n",
              "      <td>0.349715</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          GD        NR  newton-cg\n",
              "0  -0.163835 -0.163835  -0.138690\n",
              "1  -0.302044 -0.302044  -0.335076\n",
              "2  -0.063900 -0.063900  -0.008402\n",
              "3   0.223787  0.223787   0.211955\n",
              "4   0.348428  0.348428   0.310141\n",
              "5  -0.078124 -0.078124  -0.115854\n",
              "6  -0.292732 -0.292732  -0.294173\n",
              "7   0.006788  0.006788   0.009611\n",
              "8   0.089275  0.089275   0.083007\n",
              "9   0.071243  0.071243   0.102020\n",
              "10 -0.149496 -0.149496  -0.125814\n",
              "11  0.138974  0.138974   0.146051\n",
              "12  0.215181  0.215181   0.191257\n",
              "13 -0.054562 -0.054562  -0.083684\n",
              "14  0.039200  0.039200   0.026334\n",
              "15  0.208081  0.208081   0.222664\n",
              "16 -0.373804 -0.373804  -0.382235\n",
              "17 -0.002712 -0.002712  -0.055823\n",
              "18  0.132793  0.132793   0.171652\n",
              "19  0.355327  0.355327   0.349715"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDckrc4QZ8ip",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **3. Modified local quadratic approximation algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvS4kpDbZ8iq",
        "colab_type": "text"
      },
      "source": [
        "Let $f : \\mathbb{R}^p \\rightarrow \\mathbb{R}$ be a convex function  \n",
        "* **local linear approximation function $l$ of $f$ at $\\mathbf{x} = \\mathbf{x}^c$**  \n",
        "$f_{l}(\\mathbf{x}|\\mathbf{x}^c) = f(\\mathbf{x}^c) + \\nabla f(\\mathbf{x}^c)^T(\\mathbf{x}-\\mathbf{x}^c)$  \n",
        "    + $\\nabla f(\\mathbf{x}^c)$ : gradient vector of $f$ at $\\mathbf{x}^c$  \n",
        "    + gradient dexcent algorithm  $\\mathbf{x}^{\\alpha} = \\mathbf{x}^c - \\alpha \\nabla f(\\mathbf{x}^c)$, with a step size $\\alpha > 0$  \n",
        "* **local quadratic approximation function $q$ of $f$ at $\\mathbf{x} = \\mathbf{x}^c$**  \n",
        "$f_q(\\mathbf{x}|\\mathbf{x}^c) = f(\\mathbf{x}^c) + \\nabla f(\\mathbf{x}^c)^T(\\mathbf{x} - \\mathbf{x}^c) + (\\mathbf{x} - \\mathbf{x}^c)^T\\nabla^2f(\\mathbf{x}^c)(\\mathbf{x} - \\mathbf{x}^c)/2$  \n",
        "    + $\\nabla^2f(\\mathbf{x}^c)$ : Hessian matirx of $f$ at $\\mathbf{x}^c$  \n",
        "    + Newton-Raphson algorithm $\\mathbf{x}^n = \\mathbf{x}^c - \\nabla^2f(\\mathbf{x}^c)^{-1}\\nabla f(\\mathbf{x}^c) = argmin_{\\mathbf{x}}f_{q}(\\mathbf{x}|\\mathbf{x}^c)$  \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2djQG3yZ8iq",
        "colab_type": "text"
      },
      "source": [
        "Newton-Raphson Algorithm may fail to descent, that is, $f(\\mathbf{x}^{new}) > f(\\mathbf{x}^{current})$  \n",
        "* general Newton-Raphson Algorithm ; $\\mathbf{x}^{\\alpha} = \\mathbf{x}^c - \\alpha \\nabla^2 f(\\mathbf{x}^c)^{-1} \\nabla f(\\mathbf{x}^c) \\ne \\mathbf{x}^n$  , with a step size $\\alpha > 0$  \n",
        "* If $\\mathbf{x}^c$ is not the minimizer of $f$ then there exists $\\alpha > 0$ such that $f(\\mathbf{x}^{\\alpha}) < f(\\mathbf{x}^c)$  \n",
        "where $\\mathbf{x}^{\\alpha} = \\mathbf{x}^c - \\alpha \\nabla^2 f(\\mathbf{x}^c)^{-1}\\nabla f(\\mathbf{x}^c) = \\mathbf{x}^c - \\alpha(\\mathbf{x}^c - \\mathbf{x}^n)$  \n",
        "$=\\alpha \\mathbf{x}^n + (1-\\alpha)\\mathbf{x}^c$  \n",
        "    + $\\mathbf{x}^c$ is not a minimizer of $f$ so that $\\nabla f_q(\\mathbf{x}^c | \\mathbf{x}^c) = \\nabla f(\\mathbf{x}^c) \\ne \\mathbf{0}$  \n",
        "    + Hence $\\mathbf{x}^c$ is not a minimizer of $q$ which implies $f_q(\\mathbf{x}^n|\\mathbf{x}^c) < f_q(\\mathbf{x}^c | \\mathbf{x}^c)$  \n",
        "    + From the convexity of $f_q$ ,for any $\\alpha \\in (0, 1]$  \n",
        "    $f_q(\\mathbf{x}^{\\alpha}|\\mathbf{x}^c) = f_q(\\alpha \\mathbf{x}^n + (1-\\alpha)\\mathbf{x}^c | \\mathbf{x}^c) \\le \\alpha f_q(\\mathbf{x}^n | \\mathbf{x}^c) + (1-\\alpha)f_q(\\mathbf{x}^c | \\mathbf{x}^c)$  \n",
        "    + Hence $f_q(\\mathbf{x}^{\\alpha} | \\mathbf{x}^c) - f_q(\\mathbf{x}^c | \\mathbf{x}^c) \\le \\alpha(f_q(\\mathbf{x}^n|\\mathbf{x}^c) - f_q(\\mathbf{x}^c | \\mathbf{x}^c))$  \n",
        "    + using the fact that $\\nabla f(\\mathbf{x}^c) = \\nabla f_q(\\mathbf{x}^c | \\mathbf{x}^c)$,  \n",
        "    $\\lim_{\\alpha \\to \\infty}\\frac{f(\\mathbf{x}^{\\alpha}) - f(\\mathbf{x}^c)}{\\alpha} = \\lim_{\\alpha \\to \\infty}\\frac{f(\\mathbf{x}^{\\alpha}|\\mathbf{x}^c) - f(\\mathbf{x}^c|\\mathbf{x}^c)}{\\alpha} \\le f_q(\\mathbf{x}^n | \\mathbf{x}^c) - f_q(\\mathbf{x}^c|\\mathbf{x}^c) < 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc2BdzYUZ8ir",
        "colab_type": "text"
      },
      "source": [
        "## **Algorithm**  \n",
        ">1. Set an initial $\\mathbf{x}_1$  \n",
        "2. For $n \\ge 1$ ,repeat the followings until convergence.  \n",
        "    (1) Let $\\mathbf{x}_{\\alpha} = \\mathbf{x}_n -\\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x}_n)$  \n",
        "    (2) If $f(\\mathbf{x}_{\\alpha}) < f(\\mathbf{x}_n)$ then set $\\mathbf{x}_{n+1} = \\mathbf{x}_{\\alpha}$  \n",
        "    (3) Else set $\\mathbf{x}_{n+1} = \\mathbf{x}_{\\alpha}$ ,where $\\alpha = argmin_{h \\in [0, 1]}f(h\\mathbf{x}_{\\alpha} + (1-h)\\mathbf{x}_n)$"
      ]
    }
  ]
}