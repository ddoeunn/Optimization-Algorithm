{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "256px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "1. optimization of multivariate function.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khKKKTVIZ8hm",
        "colab_type": "text"
      },
      "source": [
        "# ***OPTIMIZATION OF MULTIVARIATE FUNCTIONS*** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOw7xZ5uZ8ho",
        "colab_type": "text"
      },
      "source": [
        ">* Assume that $f$ is a multivariate real function $f: \\mathbb{R}^p \\rightarrow \\mathbb{R}$  \n",
        "that has only one true or exact minimizer $\\mathbf{x}^* = argmin_{\\mathbf{x} \\in \\mathit{D}}f(\\mathbf{x})$  \n",
        "* Several algorithms for obtaining a sequence $\\{\\mathbf{x}_n\\}_{n \\ge 1}$  \n",
        "that satisfies $\\exists N \\in \\mathbb{N}$ s.t. $\\rVert\\mathbf{x}_n-\\mathbf{x}^*\\rVert < \\epsilon, \\forall n \\ge N$ for a given error bound $\\epsilon > 0$  \n",
        "* For the sequence above, put $\\hat{\\mathbf{x}} = \\mathbf{x}_N$ as an approximated minimizer of $\\mathbf{x}^*$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BObdlot8Z8hp",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **1. Coordinate Descent Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOA-HjCoZ8hq",
        "colab_type": "text"
      },
      "source": [
        "## **Algorithm**  \n",
        "* Iteratively solves univariate minimization problems for minimizing a function  \n",
        "* Given an initial value $\\mathbf{x}_1 = (x_{11}, \\cdots, x_{1p})^{T}$  \n",
        "$f(\\mathbf{x_1}) \\ge f(x_{21}, x_{12}, x_{13}, \\cdots, x_{1p}) \\ge f(x_{21}, x_{22}, x_{13}, \\cdots, x_{1p}) \\ge \\cdots \\ge f(x_{21}, x_{22}, x_{23}, \\cdots, x_{2p})$  \n",
        ",where $x_{21} = argmin_{t \\in \\mathbb{R}}f(t, x_{12}, x_{13}, \\cdots, x_{1p}), \\cdots, \n",
        "x_{2p}= argmin_{t \\in \\mathbb{R}}f(x_{21}, x_{22}, x_{23}, \\cdots, t)$   \n",
        "so that we eventually have $f(\\mathbf{x}_1) \\ge f(\\mathbf{x}_2)$ ,where $\\mathbf{x}_2 = (x_{21}, x_{22}, \\cdots, x_{2p} )^{T}$  \n",
        "\n",
        ">1. Take an initial $\\mathbf{x}_1 = (x_{11}, \\cdots, x_{1p})^{T}$  \n",
        "2. For $n = 1, 2, \\cdots, $  \n",
        "    (1) For $j=1, 2, \\cdots, p$  \n",
        "    $x_{(n+1)j} = argmin_{t \\in \\mathbb{R}}f(x_{(n+1)1}, \\cdots, x_{(n+1)(j-1)}, t, x_{n(j+1)}, \\cdots, x_{np})$  \n",
        "3. If $\\rVert\\mathbf{x}_{n+1}-\\mathbf{x}_n\\rVert < \\epsilon$ then stop, where $\\mathbf{x}_{n+1} = (x_{(n+1)1}, x_{(n+1)2}, \\cdots, x_{(n+1)p})^{T}$  \n",
        "4. Set $\\hat{\\mathbf{x}} = \\mathbf{x}_n$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlMvcYwSZ8hr",
        "colab_type": "text"
      },
      "source": [
        "* **the key step is the minimization in (1)**  \n",
        "    + If it has closed form we are lucky.  \n",
        "    + If not, we need to solve univariate minimization problem.  \n",
        "    + If $f$ is differentiable then (1) can be done by solving $\\frac{df(x_{(n+1)1}, \\cdots, x_{(n+1)(j-1)}, t, x_{n(j+1)}, \\cdots, x_{np})}{dt} = 0$  \n",
        "    \n",
        "* **If $f$ is convex but not differentiable then the algorithm may stuck in a point that is not a local minimizer**  \n",
        ": $f(x, y) = (x-y)^2 +(x+y)^2 + |x-y| + 10|x+y|$  \n",
        "\n",
        "* **Tseng(2001) proved that the CD algorithm always converges to a local minimizer**  \n",
        "**if $f(\\mathbf{x}) = g(\\mathbf{x}) + \\sum^{p}_{j=1}h_j(x_j)$ where $g$ is convex differenctiable and $h_j$ is convex but may not differentiable for all $j \\le p$**  \n",
        ": $f(x, y) = x^2 + y^2 + |x| + 10|y|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbb5XU7bZ8hr",
        "colab_type": "text"
      },
      "source": [
        "## **[Example - LSE]**  \n",
        "* Consider Linear regression model, $\\mathbf{y} = \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon}$  \n",
        ",where $\\mathbf{y} = (y_1, \\cdots, y_n)^{T}$ is a vector of response, $\\mathbf{X} = (\\mathbf{X}_1, \\cdots, \\mathbf{X}_p)$ is a design matrix, $\\mathbf{\\beta} = (\\beta_1, \\cdots, \\beta_p)^{T}$ is a parameter vector and $\\mathbf{\\epsilon} = (\\epsilon_1, \\cdots, \\epsilon_n)^{T}$ is a random error vector.  \n",
        "* **Least Square Estimator, LSE**  \n",
        ">+ $\\hat{\\mathbf{\\beta}}^{LSE} = argmin_{\\mathbf{\\beta}}L(\\mathbf{\\beta}) = argmin_{\\mathbf{\\beta}}\\rVert\\mathbf{y} - \\mathbf{X\\beta}\\rVert^2 / 2$  \n",
        "> + For each $j \\le p$,  \n",
        "$L(\\mathbf{\\beta}) = (\\mathbf{X}^{T}_j\\mathbf{X}_j)\\beta^2_j/2 - \\mathbf{X}^{T}_j(\\mathbf{y}-\\mathbf{Z}_i\\mathbf{\\alpha}_j)\\beta_j + (\\mathbf{y}-\\mathbf{Z}_j\\mathbf{\\alpha}_j)^{T}(\\mathbf{y}-\\mathbf{Z}_j\\mathbf{\\alpha}_j)/2$  \n",
        "where $\\mathbf{Z}_j = (\\mathbf{X}_j, \\cdots, \\mathbf{X}_{j-1}, \\mathbf{X}_{j+1}, \\cdots, \\mathbf{X}_p)$ and $\\mathbf{\\alpha}_j = (\\beta_1, \\cdots, \\beta_{j-1}, \\beta_{j+1}, \\cdots, \\beta_p)^{T}$  \n",
        "> + Hence, $\\hat{\\beta}^{LSE}_j = \\mathbf{X}^{T}_j(\\mathbf{y}-\\mathbf{Z}_j\\mathbf{\\alpha}_j)/\\mathbf{X}^{T}_j\\mathbf{X}_j$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VvS-_a2Z8hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq0ETSxtZ8hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lseCD(y, X, eps=1e-7, iter_max=1000):\n",
        "    loss = []\n",
        "    p = X.shape[1]\n",
        "    bvec = np.zeros(p)\n",
        "    idx = np.arange(0, p)\n",
        "    for i in range(iter_max):\n",
        "        cur_bvec = bvec.copy()\n",
        "        for j in range(p):\n",
        "            Z = X[:, idx != j]\n",
        "            avec = bvec[idx != j]\n",
        "            dn = np.sum(X[:, j]**2)\n",
        "            up = np.sum(X[:, j] * (y - np.matmul(Z, avec)))\n",
        "            bvec[j] = up / dn\n",
        "        loss.append(np.sum((y - np.matmul(X, bvec))**2))\n",
        "        if np.sum(np.abs(cur_bvec - bvec)) < eps: break\n",
        "    return bvec, loss, i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOnh7qj7Z8h0",
        "colab_type": "code",
        "colab": {},
        "outputId": "d5e41908-62d5-41d5-920e-b3cd141cecd1"
      },
      "source": [
        "n = 10\n",
        "p = 5\n",
        "bvec = np.ones(p)\n",
        "X = np.random.normal(0, 1, [n, p])\n",
        "y = np.matmul(X, bvec) + np.random.normal(0, 1, n)\n",
        "beta, loss, iteration = lseCD(y, X)\n",
        "beta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.97888093, 1.49644906, 0.5899192 , 1.47807672, 1.53874451])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS30D5puZ8h3",
        "colab_type": "code",
        "colab": {},
        "outputId": "0b18a068-7c05-4c64-fac0-740cac443f88"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzbI2f6AZ8h7",
        "colab_type": "text"
      },
      "source": [
        "## **[Example - LASSO]**  \n",
        "* **Least Absolute Selection and Shrinkage Opterator, LASSO**  \n",
        "> + $\\hat{\\mathbf{\\beta}}^{LASSO} = argmin_{\\mathbf{\\beta}}L_{\\lambda}(\\mathbf{\\beta}) = argmin_{\\mathbf{\\beta}}\\{\\rVert\\mathbf{y} - \\mathbf{X\\beta}\\rVert^2/2 + \\lambda \\rVert\\mathbf{\\beta}\\rVert_1\\}$  \n",
        ",for some constant $\\lambda > 0$, where $\\lambda$ is often called the tuning parameter.  \n",
        "> + For each $j \\le p$,  \n",
        "$L_{\\lambda}(\\mathbf{\\beta}) = (\\mathbf{X}^{T}_j\\mathbf{X}_j)\\beta^2_j/2 - \\mathbf{X}^T(\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j)\\beta_j + \\lambda|\\beta_j| + (\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j)^T(\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j)/2 + \\lambda\\rVert\\mathbf{\\alpha}_j\\rVert_1$  \n",
        "> + Hence, $\\hat{\\beta}^{LASSO}_j = S_{\\lambda/\\mathbf{X}^T_j\\mathbf{X}_j}(\\mathbf{X}^T_j(\\mathbf{y} - \\mathbf{Z}_j\\mathbf{\\alpha}_j) / \\mathbf{X}^T_j\\mathbf{X}_j) = S_{\\lambda/\\mathbf{X}^T_j\\mathbf{X}_j}(\\hat{\\beta}^{LSE}_j)$  \n",
        "where $S_{\\lambda} = sign(x)(|x| - \\lambda)_{+}$ and $c_{+} = cI(c>0)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hmq9DUHZ8h7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softThresh(x, lam):\n",
        "    ret = np.sign(x) * (np.abs(x) - lam) * (abs(x) > lam)\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IISw-_dZ8h-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lassoCD(y, X, lam, eps=1e-7, iter_max=1000):\n",
        "    loss = []\n",
        "    p = X.shape[1]\n",
        "    bvec = np.zeros(p)\n",
        "    idx = np.arange(0, p)\n",
        "    for i in range(iter_max):\n",
        "        cur_bvec = bvec.copy()\n",
        "        for j in range(p):\n",
        "            Z = X[:, idx != j]\n",
        "            avec = bvec[idx != j]\n",
        "            dn = np.sum(X[:, j]**2)\n",
        "            up = np.sum(X[:, j] * (y - np.matmul(Z, avec)))\n",
        "            bvec[j] = softThresh(up/dn, lam/dn)\n",
        "        loss.append(np.sum((y - np.matmul(X, bvec))**2))\n",
        "    return bvec, loss, i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T090bgt2Z8iC",
        "colab_type": "code",
        "colab": {},
        "outputId": "303f47be-8676-4bbd-8aa1-58b3f3d19fa2"
      },
      "source": [
        "n = 10\n",
        "p = 11\n",
        "bvec = np.ones(p)\n",
        "X = np.random.normal(0, 1, [n, p])\n",
        "y = np.matmul(X, bvec) + np.random.normal(0, 1, n)\n",
        "beta, loss, iteration = lassoCD(y, X, 1) \n",
        "beta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.29266696,  1.20623952, -0.        , -0.        ,  0.47794823,\n",
              "        1.05455985, -0.        ,  1.03838628,  0.77916843,  0.9635943 ,\n",
              "        0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW3BGahoZ8iF",
        "colab_type": "code",
        "colab": {},
        "outputId": "9f6cce2e-2351-4e93-cecc-d9b7273cea2a"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W64qoy_sZ8iI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **2. Gradient Descent Algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6-54uuKZ8iI",
        "colab_type": "text"
      },
      "source": [
        "***Moving a point for a direction with a stepsize***  \n",
        "* **Univariate function**  \n",
        ">+ The first key idea is to understand \"moving a point $x_n$ to $x_{n+1}$\" as \"choosing an equation\" between (1) and (2)  \n",
        "(1) : $x_{n+1} = x_n + \\alpha \\Rightarrow$ move to the right  \n",
        "(2) : $x_{n+1} = x_n - \\alpha \\Rightarrow$ move to the left  \n",
        "where $\\alpha >0$ is very small positive constant  \n",
        ">+ Hence, when $f$ is differentiable, $x_{n+1} = x_n - \\alpha \\nabla f(x_n)$  \n",
        ">+ Move $x_n$ for a direction $-\\nabla f(x_n)$ with a stepsize $\\alpha > 0$  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ_yjRqAZ8iJ",
        "colab_type": "text"
      },
      "source": [
        "* **Multivariate function**  \n",
        ">+ when the function $f$ is multivariate, there are infinitely many directions to move.  \n",
        ">+ **(Direction opposite to current gradient)**  \n",
        ": Assume that $\\nabla f(\\mathbf{x}) \\ne \\mathbf{0}$ for some fixed point $\\mathbf{x}$  \n",
        ": Let $\\mathbf{x}_{\\alpha} = \\mathbf{x} - \\alpha \\nabla f(\\mathbf{x})$ then by the first order Taylor's expansion around $\\mathbf{x}$, we have  \n",
        "$f(\\mathbf{x}_{\\alpha}) = f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T(\\mathbf{x}_{\\alpha}-\\mathbf{x}) + o(\\rVert \\mathbf{x}_{\\alpha}-\\mathbf{x} \\rVert)$  \n",
        "$= f(\\mathbf{x}) - \\alpha \\rVert \\nabla f(\\mathbf{x}) \\rVert^2 + o(\\alpha \\rVert \\nabla f(\\mathbf{x}) \\rVert^2)$  \n",
        "$= f(\\mathbf{x}) - \\alpha \\rVert \\nabla f(\\mathbf{x}) \\rVert^2 + o(\\alpha)$  \n",
        "$\\Rightarrow f(\\mathbf{x}_{\\alpha}) - f(\\mathbf{x}) < -\\alpha \\rVert\\nabla f(\\mathbf{x}) \\rVert^2 < 0$  \n",
        ">+ **(General descent condition)**  \n",
        ": Consider $\\mathbf{x}_{\\alpha}  = \\mathbf{x} + \\alpha \\mathbf{d}$ ,$\\forall \\alpha >0$ where the direction $\\mathbf{d}$ satisfies the descent condition where it makes an angle with $\\nabla f(\\mathbf{x})$ that is greater than 90 degrees,  \n",
        "that is, $\\nabla f(\\mathbf{x})^T\\mathbf{d} < 0$  \n",
        ": By the Taylor's expansion, $f(\\mathbf{x}_{\\alpha}) - f(\\mathbf{x}) < \\alpha \\nabla f(\\mathbf{x})^T\\mathbf{d} < 0$  \n",
        "for all sufficiently small $\\alpha > 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrxE1XJSZ8iK",
        "colab_type": "text"
      },
      "source": [
        "## **Algorithm**  \n",
        ">1. Take an initial $\\mathbf{x}_1$  \n",
        ">2. For $n = 1, 2, \\cdots, $  \n",
        "    (1) Choose a direction $\\mathbf{d}_n$ and a stepsize $\\alpha_n$ and set $\\mathbf{x}_{n+1} = \\mathbf{x}_n + \\alpha_n\\mathbf{d}_n$  \n",
        ">3. If $\\rVert \\mathbf{x}_{n+1} - \\mathbf{x}_n \\rVert < \\epsilon$ then stop.  \n",
        ">4. Set $\\hat{\\mathbf{x}} = \\mathbf{x}_n$   \n",
        "\n",
        "* **the key step is (1)**  \n",
        ": If $\\nabla f(\\mathbf{x}_n)^T\\mathbf{d}_n > 0$ then cannot guarantee the descent property.  \n",
        ": If $\\alpha_n$ is too small the convergence speed is too late.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjBbn0vLZ8iK",
        "colab_type": "text"
      },
      "source": [
        "## **Selecting the descent direction**  \n",
        ">* **General form of GD algorithm**  \n",
        ": The most popular form of the GD algorithm is specified by $\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\alpha_n\\mathbf{D}_n\\nabla f(\\mathbf{x}_n)$  \n",
        ",where $\\mathbf{D}_n$ is a strictly positive definite symmetric matrix and $\\alpha_n$ is a stepsize.  \n",
        ": $\\mathbf{d}_n = -\\mathbf{D}_n \\nabla f(\\mathbf{x})$  \n",
        "$~\\nabla f(\\mathbf{x}_n)^T\\mathbf{D}_n\\nabla f(\\mathbf{x}_n) > 0$ because of the strictly positive definiteness of $\\mathbf{D}_n$  \n",
        "\n",
        ">* **Steepest descent method**  \n",
        ": $\\mathbf{D}_n = \\mathbf{I}_p \\Rightarrow \\mathbf{x}_{n+1} = \\mathbf{x}_n - \\alpha \\nabla f(\\mathbf{x}_n)$  \n",
        "$~\\nabla f(\\mathbf{x}_n)^T\\mathbf{d}_n \\ge - \\rVert \\nabla f(\\mathbf{x}_n) \\rVert \\rVert \\mathbf{d}_n \\rVert = -\\rVert \\nabla f(\\mathbf{x}) \\rVert < 0$ ,$\\forall \\rVert\\mathbf{d}_n \\rVert = 1$ by the Schwartz inequality  \n",
        ": Hence, $\\mathbf{d}_n = -\\nabla f(\\mathbf{x}_n) / \\rVert\\nabla f(\\mathbf{x}_n) \\rVert$ where the equality holds for the Schwartz inequality  \n",
        ": the gradient direction is almost orthogonal to the direction for the minimum. Hence, the GD algorithm shows 'zig-zagging' behavior without making fast progress.  \n",
        "\n",
        ">* **Newton's method**  \n",
        ": Assume that $f$ is convex and twice differentiable  \n",
        ": $\\mathbf{D}_n = \\nabla^2f(\\mathbf{x}_n)^{-1} \\Rightarrow \\mathbf{x}_{n+1} = \\mathbf{x}_n -\\alpha_n \\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x})$ ,where $\\nabla^2f(\\mathbf{x}_n)$ is strictly positive definite  \n",
        ": typically converges vary fast and does not exhibit 'zig-zagging' behavior  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLvNwaLpZ8iL",
        "colab_type": "text"
      },
      "source": [
        "## **Newton-Raphson**  \n",
        "* Newton-Raphson algorithm is a gradient descent algorithm when $\\alpha_n = 1$ ,$\\mathbf{D}_n = \\nabla^2f(\\mathbf{x}_n)^{-1} \\Rightarrow \\mathbf{x}_{n+1} = \\mathbf{x}_n - \\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x})$  \n",
        "* Suppose $f : \\mathit{D} \\rightarrow \\mathbb{R}^p$ is convex and twice differentiable, has second derivaton on $\\mathit{D}$  \n",
        "    + The second order approximation of $f$ at $\\mathbf{x}_0$ becomes  \n",
        "    $f(\\mathbf{x}) \\approx q(\\mathbf{x}) = f(\\mathbf{x}_0) + \\nabla f(\\mathbf{x}_0)^T(\\mathbf{x} - \\mathbf{x}_0) +(\\mathbf{x} - \\mathbf{x}_0)^T\\nabla^2f(\\mathbf{x}_0)(\\mathbf{x} - \\mathbf{x}_0)/2$ around $\\mathbf{x}_n$  \n",
        "    ,where $\\nabla f(\\mathbf{x}_0)$ is the gradient vector of $f$ at $\\mathbf{x}_0$ and  $\\nabla^2f(\\mathbf{x}_0)$ is the Hessian matrix of $f$ at $\\mathbf{x}_0$  \n",
        "    + Minimizing the quadratic approximation $q$ around $\\mathbf{x}_n$  \n",
        "    + By taking the derivative to zero,  \n",
        "    $\\nabla f(\\mathbf{x}_n) = \\nabla^2f(\\mathbf{x}_n)(\\mathbf{x} - \\mathbf{x}_n) = 0  \\Leftrightarrow \\mathbf{x} = \\mathbf{x}_n - \\nabla f(\\mathbf{x}_n)^{-1} \\nabla f(\\mathbf{x}_n)$  \n",
        "* Hence, the Newton-Raphson Algorithm can be thought of a successive minimization of quadratic approximations \n",
        "$\\mathbf{x}_{n+1} = \\mathbf{x}_n - \\alpha \\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPC_UG1XZ8iL",
        "colab_type": "text"
      },
      "source": [
        "## **[Example - LogisticRegression]**  \n",
        "* The estimator(mle) is the minimizer of the logistic loss:  \n",
        "$\\hat{\\mathbf{\\beta}} = argmin_{\\mathbf{\\beta}}L(\\mathbf{\\beta})= argmin_{\\mathbf{\\beta}}\\sum^{n}_{i=1}\\{-y_i \\mathbf{x}^T_i \\mathbf{\\beta} +log(1+exp(\\mathbf{x}^T_i\\mathbf{\\beta}))\\}$  \n",
        "which can be obtained by modeling $f(\\mathbf{x}^T_i\\mathbf{\\beta}) = exp(\\mathbf{x}^T_i\\mathbf{\\beta}) / \\{ 1+exp(\\mathbf{x}^T_i\\mathbf{\\beta})\\}$  \n",
        "* $\\nabla L(\\mathbf{\\beta}) = \\sum^{n}_{i=1}[-y_i + exp(\\mathbf{x}^T_i\\mathbf{\\beta}) / \\{1+exp(\\mathbf{x}^T_i\\mathbf{\\beta})\\}]\\mathbf{x}_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjxgysxuZ8iM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getLoss(y, X, bvec):\n",
        "    xbvec = np.matmul(X, bvec)\n",
        "    loss = -np.sum(y * xbvec) + np.sum(np.log(1 + np.exp(xbvec)))\n",
        "    return loss\n",
        "\n",
        "def getGrad(y, X, bvec):\n",
        "    xbvec = np.minimum(np.matmul(X, bvec), 700)\n",
        "    exbvec = np.exp(xbvec)\n",
        "    pvec = exbvec / (1 + exbvec)\n",
        "    grad = np.matmul(np.transpose(X), (pvec - y))\n",
        "    return grad\n",
        "\n",
        "def logisticGD(y, X, bvec, eps=1e-7, iter_max=1000):\n",
        "    losses = []\n",
        "    for i in range(iter_max):\n",
        "        alpha = 1\n",
        "        gvec = getGrad(y, X, bvec)\n",
        "        loss = getLoss(y, X, bvec)\n",
        "        losses.append(loss)\n",
        "        # stepsize from the backtracking rule\n",
        "        for j in range(iter_max):\n",
        "            nbvec = bvec - alpha * gvec # new bvec\n",
        "            if getLoss(y, X, nbvec) < loss: break\n",
        "            alpha = alpha / 2\n",
        "        if np.sum(np.abs(nbvec - bvec)) < eps: break\n",
        "        bvec = nbvec.copy()\n",
        "    return bvec, losses, i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLcd0AFMZ8iP",
        "colab_type": "code",
        "colab": {},
        "outputId": "882378d6-69f9-43ec-c84d-d767f205d4ee"
      },
      "source": [
        "n = 100\n",
        "p = 20\n",
        "X = np.random.normal(0, 1, [n, p])\n",
        "y = np.random.choice([0, 1], n, replace=True)\n",
        "bvec = np.zeros(p)\n",
        "betaGD, loss, iteration = logisticGD(y, X, bvec)\n",
        "betaGD"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.21986229, -0.24421996, -0.1642458 , -0.06291972,  0.18175028,\n",
              "       -0.24288292, -0.33729176,  0.24733213, -0.31099921, -0.09760213,\n",
              "       -0.11526062,  0.54991669, -0.25218506,  0.0630333 , -0.32395103,\n",
              "        0.58067469, -0.12044125,  0.06332383, -0.62311939, -0.27509036])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xHzPnZ5Z8iS",
        "colab_type": "code",
        "colab": {},
        "outputId": "a2074774-1562-491c-cc13-1d3e7d966c5e"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph GD')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAVZklEQVR4nO3df7RddXnn8ffHJGhE8EJzUQhgoGoYVxWCV0dbpRbsxHFaoYzOdFbtoMNamTqVVRybEWpHg6td/ki1ZWZNbamIrBZrNQNoZy2JjNM62hnRRKKxhohSlCQioUwUMAjEZ/44++rNzf1xktwf53vv+7VW1jn7u7/77Ie9yOfsPHufc1JVSJLa84T5LkCSdGQMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngmnVJ7k7y8vmuYzYs5P82DT4DXJpDSU5O8mdJ9iR5KMldST6U5Kxu/aok1a17KMl3k/yPJL8437Vr8BjgWrSSLJ3j/f0U8H+AJwMvBY4DzgU+A4wP6KGqegpwNnArcFOS181dtWqBAa45leSJSf6oOwPd0z1/YrduRXe2uS/JA0k+m+QJ3bq3JNmd5MEkO5NcMMnr/1SSv07y/SRfTPJ7ST43Zn0l+c0kdwJ3dmNXJ7mn22ZrkpeOmb8hyaYkf9Xt+0tJzh6323OSfCXJ97p5T5rkP/9NwPeBX6+qb1bPvqq6rqr+60QbVNW9VXU1sAF49+jxkMAA19x7K/Ai4Bx6Z5cvBH63W/dmYBcwDDwN+B2gkqwG3gi8oKqOA9YCd0/y+v8NeBh4OnBJ92e8i4B/CjynW/5iV8+JwIeBj40L4QuBj41Zf3OSZWPW/yvgFcAZwPOA101S28uBm6rqR5Osn8qNwEnA6iPYVguUAa659mvAO6rqvqraC1wF/Hq37jHgZOAZVfVYVX22el/WcwB4IvCcJMuq6u6q+ub4F06yBPiXwNur6gdV9TXg+glqeGdVPVBV+wGq6i+q6h+r6vGqem+3r7FBubWqNlXVY8D7gCfRexMa9V+qak9VPQD8Nb03g4msAO4dU++run9tPJjkU1MdNGBP93jiNPO0iBjgmmunAN8as/ytbgxgI/AN4FPdxb0rAKrqG8Dl9NoI9yX5SJJTONQwsBS4Z8zYPRPMO2gsyZuT7OhaIPuAp9IL20Pmd2fPu8bUDGNCGfgB8JQJ9gnwj/TeoEZf6xNVNUSvtXLMJNuMWtk9PjDNPC0iBrjm2h7gGWOWT+/GqKoHq+rNVXUm8MvAfxztdVfVh6vqJd22Bbx7gtfeCzwOnDpm7LQJ5v34Kzi7fvdb6LVBTugC9XtAJnqNrgd9Kj85Iz4cnwYuOsI+9q8A9wE7j2BbLVAGuObaXwK/m2Q4yQrgbcBfACT5pSTPTBJ6F/sOAAeSrE5yfnex8xFgf7fuIFV1gF6veEOSJ3e35v3baeo5jl7o7wWWJnkbcPy4Oc9PcnF318rlwA+Bzx/Bf/v7gBOAP0/y0+k5jslbLiR5WpI3Am8HrjzC/rkWKANcc+33gC3AV4DtwJe6MYBnAf8TeAj4v8AfV9Xf0utJvwu4n1674iR6Fzgn8kZ6LZB7gT+n94bxwynq2Qx8Evg6vXbOIxzadvk48K+B/0evX39x1w8/LFV1P73e+SPA54AHgW303kTeMG76viQP0ztGrwReU1UfPNx9amGLP+ighSzJu4GnV9VEd6P0s/0G4JlV9doZLUyaAZ6Ba0FJclaS53XtiRcClwI3zXdd0myY00+iSXPgOHptk1PoXfR7L70WiLTg2EKRpEbZQpGkRs1pC2XFihW1atWqudylJDVv69at91fV8PjxOQ3wVatWsWXLlrncpSQ1L8m3Jhq3hSJJjTLAJalRBrgkNcoAl6RGGeCS1KiB/yTmzbfvZuPmnezZt59Thpazfu1qLlqzcvoNJWmBG+gAv/n23Vx543b2P9b75tDd+/Zz5Y3bAQxxSYveQLdQNm7e+ePwHrX/sQNs3Ox32kvSQAf4nn37D2tckhaTgQ7wU4aWH9a4JC0mAx3g69euZvmyJQeNLV+2hPVrV0+yhSQtHgN9EXP0QqV3oUjSoQY6wKEX4ga2JB1qoFsokqTJGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1qq8ATzKUZFOSO5LsSPLiJOck+XySbUm2JHnhbBcrSfqJfn+R52rglqp6dZJjgCcDHwWuqqpPJnkl8B7gZbNTpiRpvGkDPMnxwHnA6wCq6lHg0SQFHN9NeyqwZ5ZqlCRNoJ8z8DOBvcB1Sc4GtgK/BVwObE7yB/RaMT870cZJ1gHrAE4//fSZqFmSRH898KXAucD7q2oN8DBwBfAG4E1VdRrwJuDaiTauqmuqaqSqRoaHh2eobElSPwG+C9hVVbd1y5voBfolwI3d2McAL2JK0hyaNsCr6l7gniSru6ELgK/R63n/fDd2PnDnrFQoSZpQv3ehXAbc0N2BchfweuDjwNVJlgKP0PW5JUlzo68Ar6ptwMi44c8Bz5/xiiRJffGTmJLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSo/oK8CRDSTYluSPJjiQv7sYvS7Izyd8nec/slipJGmtpn/OuBm6pqlcnOQZ4cpJfAC4EnldVP0xy0qxVKUk6xLQBnuR44DzgdQBV9SjwaJI3AO+qqh924/fNYp2SpHH6aaGcCewFrktye5IPJDkWeDbw0iS3JflMkhfMaqWSpIP0E+BLgXOB91fVGuBh4Ipu/ATgRcB64KNJMn7jJOuSbEmyZe/evTNXuSQtcv0E+C5gV1Xd1i1vohfou4Abq+cLwI+AFeM3rqprqmqkqkaGh4dnqm5JWvSmDfCquhe4J8nqbugC4GvAzcD5AEmeDRwD3D9LdUqSxun3LpTLgBu6O1DuAl5Pr5XywSRfBR4FLqmqmp0yJUnj9RXgVbUNGJlg1WtnthxJUr/8JKYkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIa1VeAJxlKsinJHUl2JHnxmHW/naSSrJi9MiVJ4y3tc97VwC1V9eokxwBPBkhyGvCLwLdnqT5J0iSmPQNPcjxwHnAtQFU9WlX7utV/CPwnoGatQknShPppoZwJ7AWuS3J7kg8kOTbJq4DdVfXlqTZOsi7JliRb9u7dOxM1S5LoL8CXAucC76+qNcDDwAbgrcDbptu4qq6pqpGqGhkeHj6aWiVJY/QT4LuAXVV1W7e8iV6gnwF8OcndwKnAl5I8fVaqlCQdYtoAr6p7gXuSrO6GLgC+VFUnVdWqqlpFL+TP7eZKkuZAv3ehXAbc0N2Bchfw+tkrSZLUj74CvKq2ASNTrF81UwVJkvrjJzElqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSo/q9D3yg3Xz7bjZu3smeffs5ZWg569eu5qI1K+e7LEmaVc0H+M237+bKG7ez/7EDAOzet58rb9wOYIhLWtCab6Fs3Lzzx+E9av9jB9i4eec8VSRJc6P5AN+zb/9hjUvSQtF8gJ8ytPywxiVpoWg+wNevXc3yZUsOGlu+bAnr166eZAtJWhiav4g5eqHSu1AkLTbNBzj0QtzAlrTYNN9CkaTFygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGLYiP0vfDX+2RtNAsigD3V3skLUSLooXir/ZIWoj6CvAkQ0k2JbkjyY4kL06ysVv+SpKbkgzNdrFHyl/tkbQQ9XsGfjVwS1WdBZwN7ABuBX6mqp4HfB24cnZKPHr+ao+khWjaAE9yPHAecC1AVT1aVfuq6lNV9Xg37fPAqbNX5tHxV3skLUT9nIGfCewFrktye5IPJDl23Jx/B3xyoo2TrEuyJcmWvXv3HmW5R+aiNSt558XPZeXQcgKsHFrOOy9+rhcwJTUtVTX1hGSE3hn2z1XVbUmuBr5fVf+5W/9WYAS4uKZ5sZGRkdqyZcvMVC5Ji0SSrVU1Mn68nzPwXcCuqrqtW94EnNu96CXALwG/Nl14S5Jm1rQBXlX3AvckGW0YXwB8LckrgLcAr6qqH8xijZKkCfT7QZ7LgBuSHAPcBbwe+CLwRODWJACfr6rfmJUqJUmH6CvAq2obvT73WM+c+XIkSf1aFJ/ElKSFyACXpEYZ4JLUKANckhplgEtSowxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1CgDXJIaZYBLUqMMcElqlAEuSY0ywCWpUQa4JDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUqL4CPMlQkk1J7kiyI8mLk5yY5NYkd3aPJ8x2sZKkn+j3DPxq4JaqOgs4G9gBXAF8uqqeBXy6W5YkzZFpAzzJ8cB5wLUAVfVoVe0DLgSu76ZdD1w0W0VKkg7Vzxn4mcBe4Loktyf5QJJjgadV1XcAuseTJto4ybokW5Js2bt374wVLkmLXT8BvhQ4F3h/Va0BHuYw2iVVdU1VjVTVyPDw8BGWKUkar58A3wXsqqrbuuVN9AL9u0lOBuge75udEiVJE5k2wKvqXuCeJKu7oQuArwGfAC7pxi4BPj4rFUqSJrS0z3mXATckOQa4C3g9vfD/aJJLgW8Dr5mdEiVJE+krwKtqGzAywaoLZrYcSVK//CSmJDXKAJekRhngktQoA1ySGmWAS1KjDHBJapQBLkmNMsAlqVEGuCQ1ygCXpEYZ4JLUKANckhplgEtSowxwSWqUAS5Jjer3Bx0WvJtv383GzTvZs28/pwwtZ/3a1Vy0ZuV8lyVJkzLA6YX3lTduZ/9jBwDYvW8/V964HcAQlzSwbKEAGzfv/HF4j9r/2AE2bt45TxVJ0vQMcGDPvv2HNS5Jg8AWCnDK0HJ2TxDWpwwtP2i5nz65vXRJc8UAB9avXX1QDxxg+bIlrF+7+sfL/fTJ+50zE28C082Zq/0MUi3up/1aFuN+jsaSDRs2zNiLTeeaa67ZsG7dujnbX7/OOvl4Tj1hOdt3f4+HHnmclUPLedsvP+egA33p9Vt44AePHrTd4z8qtu/+Hpe+5Iy+5owG/OicBx95nM98fS+nnrCcs04+HmBG5szVfgapFvfTfi2LcT/9uuqqq76zYcOGa8aP2wPvXLRmJX93xfn8w7v+BX93xfmHvEv20yefbk4/F0tnYs5c7WeQanE/7deyGPdztAzwPo3vh080Pt2cmXgT6GfOXO1nkGpxP+3Xshj3c7QM8D6tX7ua5cuWHDQ2vk8+3ZyZeBPoZ85c7WeQanE/7deyGPdztAzwPl20ZiXvvPi5rBxaToCVQ8t558XPPajVMt2cmXgT6GfOXO1nkGpxP+3Xshj3c7S8iHkYzjr5eC59yRlc/vJnc+lLzpjwQsRUc/q5WDoTc+ZqP4NUi/tpv5bFuJ9+TXYRM1U17cZJ7gYeBA4Aj1fVSJJzgD8BngQ8DvyHqvrCVK8zMjJSW7ZsOeziJWkxS7K1qkbGjx/OfeC/UFX3j1l+D3BVVX0yySu75ZcdXZmSpH4dTQ+8gNH+wFOBPUdfjiSpX/2egRfwqSQF/GlVXQNcDmxO8gf03gh+dqINk6wD1gGcfvrpR1+xJAno/wz856rqXOCfA7+Z5DzgDcCbquo04E3AtRNtWFXXVNVIVY0MDw/PSNGSpD4DvKr2dI/3ATcBLwQuAW7spnysG5MkzZFpWyhJjgWeUFUPds//GfAOej3vnwf+FjgfuHO619q6dev9Sb51hLWuAO6fdtbgaKnelmqFtuptqVZoq97FVOszJhrspwf+NOCmJKPzP1xVtyR5CLg6yVLgEbo+91Sq6oh7KEm2THQbzaBqqd6WaoW26m2pVmirXmvtI8Cr6i7g7AnGPwc8f6YLkiT1x4/SS1KjWgrwQz5GOuBaqrelWqGteluqFdqqd9HX2tdH6SVJg6elM3BJ0hgGuCQ1qokAT/KKJDuTfCPJFfNdz1SS3J1ke5JtSQbuqxeTfDDJfUm+OmbsxCS3JrmzezxhPmscNUmtG5Ls7o7vtu6L1AZCktOS/E2SHUn+PslvdeMDd3ynqHXgjm+SJyX5QpIvd7Ve1Y2fkeS27rj+VZJj5rtWmLLeDyX5hzHH9pyj3llVDfQfYAnwTeBM4Bjgy8Bz5ruuKeq9G1gx33VMUd95wLnAV8eMvQe4ont+BfDu+a5zilo3AL8937VNUu/JwLnd8+OArwPPGcTjO0WtA3d8gQBP6Z4vA24DXgR8FPjVbvxPgDfMd63T1Psh4NUzua8WzsBfCHyjqu6qqkeBjwAXznNNzaqq/w08MG74QuD67vn1wEVzWtQkJql1YFXVd6rqS93zB4EdwEoG8PhOUevAqZ6HusVl3Z+i9wnwTd34QBxXmLLeGddCgK8E7hmzvIsB/R+tM/rNjVu7b2JswdOq6jvQ+4sNnDTP9UznjUm+0rVY5r0dMZEkq4A19M6+Bvr4jqsVBvD4JlmSZBtwH3ArvX+V76uqx7spA5UL4+utqtFj+/vdsf3DJE882v20EOCZYGyQ732c6JsbNXPeD/w0cA7wHeC981vOoZI8BfjvwOVV9f35rmcqE9Q6kMe3qg5U1TnAqfT+Vf5PJpo2t1VNbny9SX4GuBI4C3gBcCLwlqPdTwsBvgs4bczyqQzwj0fUxN/cOOi+m+RkgO7xvnmuZ1JV9d3uL8ePgD9jwI5vkmX0AvGGqhr9ts6BPL4T1Trox7eq9tH7Ar0XAUPddzHBgObCmHpf0bWtqqp+CFzHDBzbFgL8i8CzuivOxwC/CnxinmuaUJJjkxw3+pzeNzd+deqtBsIn6H09MN3jx+exlimNBmHnVxig45veN75dC+yoqveNWTVwx3eyWgfx+CYZTjLUPV8OvJxez/5vgFd30wbiuMKk9d4x5k089Pr1R31sm/gkZncr0x/RuyPlg1X1+/Nc0oSSnEnvrBt+8s2NA1Vrkr+k99ulK4DvAm8HbqZ3Rf904NvAa6pq3i8eTlLry+j9877o3fHz70f7y/MtyUuAzwLbgR91w79Dr7c8UMd3ilr/DQN2fJM8j95FyiX0Tjo/WlXv6P6+fYReO+J24LXd2e28mqLe/wUM02sLbwN+Y8zFziPbVwsBLkk6VAstFEnSBAxwSWqUAS5JjTLAJalRBrgkNcoAl6RGGeCS1Kj/D9Ni2WuzhvawAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxH579NRZ8iU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getHess(y, X, bvec):\n",
        "    xbvec = np.minimum(np.matmul(X, bvec), 700)\n",
        "    exbvec = np.exp(xbvec)\n",
        "    pvec = exbvec / (1 + exbvec)\n",
        "    wvec = pvec * (1 - pvec)\n",
        "    hess = np.matmul(np.matmul(np.transpose(X), np.diag(wvec)), X)\n",
        "    return hess"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2McCgbZkZ8iX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logisticNR(y, X, bvec, eps=1e-7, iter_max=1000):\n",
        "    loss = []\n",
        "    for i in range(iter_max):\n",
        "        loss.append(getLoss(y, X, bvec))\n",
        "        hess = getHess(y, X, bvec)\n",
        "        grad = getGrad(y, X, bvec)\n",
        "        nbvec = bvec - np.matmul(np.linalg.inv(hess), grad) # new bvec\n",
        "        if np.sum(np.abs(nbvec - bvec)) < eps: break\n",
        "        bvec = nbvec\n",
        "    return bvec, loss, i"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu7hHAbbZ8ia",
        "colab_type": "code",
        "colab": {},
        "outputId": "8be879f9-a26e-4661-a178-4d09b967e545"
      },
      "source": [
        "betaNR, loss, iteration = logisticNR(y, X, bvec)\n",
        "betaNR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.21986231, -0.24421997, -0.1642458 , -0.06291973,  0.18175029,\n",
              "       -0.24288293, -0.33729175,  0.24733214, -0.31099922, -0.09760213,\n",
              "       -0.11526063,  0.5499167 , -0.25218507,  0.0630333 , -0.32395102,\n",
              "        0.58067469, -0.12044125,  0.06332384, -0.62311941, -0.27509037])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7oKYvbeZ8id",
        "colab_type": "code",
        "colab": {},
        "outputId": "7adb1c68-c5b1-4e79-e911-975707e7300f"
      },
      "source": [
        "plt.scatter(range(iteration + 1), loss)\n",
        "plt.title('loss graph NR')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbSq2NCWZ8ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UmdlU62Z8ik",
        "colab_type": "code",
        "colab": {},
        "outputId": "51ab8a78-52ff-4e7b-d3fc-ab9b4873ee53"
      },
      "source": [
        "clf = LogisticRegression(penalty='none', solver='newton-cg')\n",
        "clf.fit(X, y)\n",
        "betaLR = clf.coef_.squeeze()\n",
        "betaLR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.24081419, -0.24251558, -0.17889902, -0.07141892,  0.19928645,\n",
              "       -0.23724011, -0.32752683,  0.24444699, -0.29836145, -0.12410976,\n",
              "       -0.10930502,  0.54399663, -0.23151879,  0.06068003, -0.31219749,\n",
              "        0.62430111, -0.11412448,  0.04836757, -0.60472988, -0.27958893])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SteSrLXEZ8im",
        "colab_type": "code",
        "colab": {},
        "outputId": "90cc654e-47e0-4a45-adc6-b2d8c7f81333"
      },
      "source": [
        "pd.DataFrame([betaGD, betaNR, betaLR], index=['GD', 'NR', 'newton-cg']).transpose()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>GD</th>\n",
              "      <th>NR</th>\n",
              "      <th>newton-cg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.219862</td>\n",
              "      <td>-0.219862</td>\n",
              "      <td>-0.240814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.244220</td>\n",
              "      <td>-0.244220</td>\n",
              "      <td>-0.242516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.164246</td>\n",
              "      <td>-0.164246</td>\n",
              "      <td>-0.178899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.062920</td>\n",
              "      <td>-0.062920</td>\n",
              "      <td>-0.071419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.181750</td>\n",
              "      <td>0.181750</td>\n",
              "      <td>0.199286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.242883</td>\n",
              "      <td>-0.242883</td>\n",
              "      <td>-0.237240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>-0.337292</td>\n",
              "      <td>-0.337292</td>\n",
              "      <td>-0.327527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.247332</td>\n",
              "      <td>0.247332</td>\n",
              "      <td>0.244447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.310999</td>\n",
              "      <td>-0.310999</td>\n",
              "      <td>-0.298361</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>-0.097602</td>\n",
              "      <td>-0.097602</td>\n",
              "      <td>-0.124110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>-0.115261</td>\n",
              "      <td>-0.115261</td>\n",
              "      <td>-0.109305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.549917</td>\n",
              "      <td>0.549917</td>\n",
              "      <td>0.543997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>-0.252185</td>\n",
              "      <td>-0.252185</td>\n",
              "      <td>-0.231519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.063033</td>\n",
              "      <td>0.063033</td>\n",
              "      <td>0.060680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>-0.323951</td>\n",
              "      <td>-0.323951</td>\n",
              "      <td>-0.312197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.580675</td>\n",
              "      <td>0.580675</td>\n",
              "      <td>0.624301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>-0.120441</td>\n",
              "      <td>-0.120441</td>\n",
              "      <td>-0.114124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.063324</td>\n",
              "      <td>0.063324</td>\n",
              "      <td>0.048368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>-0.623119</td>\n",
              "      <td>-0.623119</td>\n",
              "      <td>-0.604730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>-0.275090</td>\n",
              "      <td>-0.275090</td>\n",
              "      <td>-0.279589</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          GD        NR  newton-cg\n",
              "0  -0.219862 -0.219862  -0.240814\n",
              "1  -0.244220 -0.244220  -0.242516\n",
              "2  -0.164246 -0.164246  -0.178899\n",
              "3  -0.062920 -0.062920  -0.071419\n",
              "4   0.181750  0.181750   0.199286\n",
              "5  -0.242883 -0.242883  -0.237240\n",
              "6  -0.337292 -0.337292  -0.327527\n",
              "7   0.247332  0.247332   0.244447\n",
              "8  -0.310999 -0.310999  -0.298361\n",
              "9  -0.097602 -0.097602  -0.124110\n",
              "10 -0.115261 -0.115261  -0.109305\n",
              "11  0.549917  0.549917   0.543997\n",
              "12 -0.252185 -0.252185  -0.231519\n",
              "13  0.063033  0.063033   0.060680\n",
              "14 -0.323951 -0.323951  -0.312197\n",
              "15  0.580675  0.580675   0.624301\n",
              "16 -0.120441 -0.120441  -0.114124\n",
              "17  0.063324  0.063324   0.048368\n",
              "18 -0.623119 -0.623119  -0.604730\n",
              "19 -0.275090 -0.275090  -0.279589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDckrc4QZ8ip",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **3. Modified local quadratic approximation algorithm**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvS4kpDbZ8iq",
        "colab_type": "text"
      },
      "source": [
        "Let $f : \\mathbb{R}^p \\rightarrow \\mathbb{R}$ be a convex function  \n",
        "* **local linear approximation function $l$ of $f$ at $\\mathbf{x} = \\mathbf{x}^c$**  \n",
        "$f_{l}(\\mathbf{x}|\\mathbf{x}^c) = f(\\mathbf{x}^c) + \\nabla f(\\mathbf{x}^c)^T(\\mathbf{x}-\\mathbf{x}^c)$  \n",
        "    + $\\nabla f(\\mathbf{x}^c)$ : gradient vector of $f$ at $\\mathbf{x}^c$  \n",
        "    + gradient dexcent algorithm  $\\mathbf{x}^{\\alpha} = \\mathbf{x}^c - \\alpha \\nabla f(\\mathbf{x}^c)$, with a step size $\\alpha > 0$  \n",
        "* **local quadratic approximation function $q$ of $f$ at $\\mathbf{x} = \\mathbf{x}^c$**  \n",
        "$f_q(\\mathbf{x}|\\mathbf{x}^c) = f(\\mathbf{x}^c) + \\nabla f(\\mathbf{x}^c)^T(\\mathbf{x} - \\mathbf{x}^c) + (\\mathbf{x} - \\mathbf{x}^c)^T\\nabla^2f(\\mathbf{x}^c)(\\mathbf{x} - \\mathbf{x}^c)/2$  \n",
        "    + $\\nabla^2f(\\mathbf{x}^c)$ : Hessian matirx of $f$ at $\\mathbf{x}^c$  \n",
        "    + Newton-Raphson algorithm $\\mathbf{x}^n = \\mathbf{x}^c - \\nabla^2f(\\mathbf{x}^c)^{-1}\\nabla f(\\mathbf{x}^c) = argmin_{\\mathbf{x}}f_{q}(\\mathbf{x}|\\mathbf{x}^c)$  \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2djQG3yZ8iq",
        "colab_type": "text"
      },
      "source": [
        "Newton-Raphson Algorithm may fail to descent, that is, $f(\\mathbf{x}^{new}) > f(\\mathbf{x}^{current})$  \n",
        "* general Newton-Raphson Algorithm ; $\\mathbf{x}^{\\alpha} = \\mathbf{x}^c - \\alpha \\nabla^2 f(\\mathbf{x}^c)^{-1} \\nabla f(\\mathbf{x}^c) \\ne \\mathbf{x}^n$  , with a step size $\\alpha > 0$  \n",
        "* If $\\mathbf{x}^c$ is not the minimizer of $f$ then there exists $\\alpha > 0$ such that $f(\\mathbf{x}^{\\alpha}) < f(\\mathbf{x}^c)$  \n",
        "where $\\mathbf{x}^{\\alpha} = \\mathbf{x}^c - \\alpha \\nabla^2 f(\\mathbf{x}^c)^{-1}\\nabla f(\\mathbf{x}^c) = \\mathbf{x}^c - \\alpha(\\mathbf{x}^c - \\mathbf{x}^n)$  \n",
        "$=\\alpha \\mathbf{x}^n + (1-\\alpha)\\mathbf{x}^c$  \n",
        "    + $\\mathbf{x}^c$ is not a minimizer of $f$ so that $\\nabla f_q(\\mathbf{x}^c | \\mathbf{x}^c) = \\nabla f(\\mathbf{x}^c) \\ne \\mathbf{0}$  \n",
        "    + Hence $\\mathbf{x}^c$ is not a minimizer of $q$ which implies $f_q(\\mathbf{x}^n|\\mathbf{x}^c) < f_q(\\mathbf{x}^c | \\mathbf{x}^c)$  \n",
        "    + From the convexity of $f_q$ ,for any $\\alpha \\in (0, 1]$  \n",
        "    $f_q(\\mathbf{x}^{\\alpha}|\\mathbf{x}^c) = f_q(\\alpha \\mathbf{x}^n + (1-\\alpha)\\mathbf{x}^c | \\mathbf{x}^c) \\le \\alpha f_q(\\mathbf{x}^n | \\mathbf{x}^c) + (1-\\alpha)f_q(\\mathbf{x}^c | \\mathbf{x}^c)$  \n",
        "    + Hence $f_q(\\mathbf{x}^{\\alpha} | \\mathbf{x}^c) - f_q(\\mathbf{x}^c | \\mathbf{x}^c) \\le \\alpha(f_q(\\mathbf{x}^n|\\mathbf{x}^c) - f_q(\\mathbf{x}^c | \\mathbf{x}^c))$  \n",
        "    + using the fact that $\\nabla f(\\mathbf{x}^c) = \\nabla f_q(\\mathbf{x}^c | \\mathbf{x}^c)$,  \n",
        "    $\\lim_{\\alpha \\to \\infty}\\frac{f(\\mathbf{x}^{\\alpha}) - f(\\mathbf{x}^c)}{\\alpha} = \\lim_{\\alpha \\to \\infty}\\frac{f(\\mathbf{x}^{\\alpha}|\\mathbf{x}^c) - f(\\mathbf{x}^c|\\mathbf{x}^c)}{\\alpha} \\le f_q(\\mathbf{x}^n | \\mathbf{x}^c) - f_q(\\mathbf{x}^c|\\mathbf{x}^c) < 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc2BdzYUZ8ir",
        "colab_type": "text"
      },
      "source": [
        "## **Algorithm**  \n",
        ">1. Set an initial $\\mathbf{x}_1$  \n",
        "2. For $n \\ge 1$ ,repeat the followings until convergence.  \n",
        "    (1) Let $\\mathbf{x}_{\\alpha} = \\mathbf{x}_n -\\nabla^2f(\\mathbf{x}_n)^{-1}\\nabla f(\\mathbf{x}_n)$  \n",
        "    (2) If $f(\\mathbf{x}_{\\alpha}) < f(\\mathbf{x}_n)$ then set $\\mathbf{x}_{n+1} = \\mathbf{x}_{\\alpha}$  \n",
        "    (3) Else set $\\mathbf{x}_{n+1} = \\mathbf{x}_{\\alpha}$ ,where $\\alpha = argmin_{h \\in [0, 1]}f(h\\mathbf{x}_{\\alpha} + (1-h)\\mathbf{x}_n)$"
      ]
    }
  ]
}